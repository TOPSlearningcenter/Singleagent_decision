## 1 ç®—æ³•ä»‹ç»
æ­¤é¡¹ç›®é‡‡ç”¨ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆ DQNï¼ˆDeep Q-Networkï¼‰ç®—æ³•ï¼‰å®ç°è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨é«˜é€Ÿå…¬è·¯ï¼ˆhighwayï¼‰ä¸Šçš„å†³ç­–ä»»åŠ¡ã€‚
### 1.1 æ¦‚è¿°
DQNï¼ˆæ·±åº¦ Q ç½‘ç»œï¼‰æ˜¯ä¸€ç§ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ï¼Œå°†ä¼ ç»Ÿçš„ Q-learning ç®—æ³•ä¸æ·±åº¦ç¥ç»ç½‘ç»œç›¸ç»“åˆã€‚

å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è®©æ™ºèƒ½ä½“ï¼ˆagentï¼‰é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜çš„å†³ç­–ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ã€‚
DQN é€šè¿‡ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥é€¼è¿‘ Q å€¼å‡½æ•°ï¼Œä½¿å¾—ç®—æ³•èƒ½å¤Ÿåœ¨å¤æ‚çš„é«˜ç»´çŠ¶æ€ç©ºé—´ä¸­å·¥ä½œï¼Œä¾‹å¦‚è§†é¢‘æ¸¸æˆæˆ–è‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥é—®é¢˜ã€‚
### 1.2 ç›¸å…³åŸç†
DQN åŸºäº Q-learningç®—æ³•è¿›è¡Œæ”¹è¿›ï¼Œæ˜¯ä¸€ç§å€¼å‡½æ•°æ–¹æ³•ï¼Œç”¨äºä¼°è®¡çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ˆstate-action pairï¼‰çš„æœŸæœ›æœªæ¥å¥–åŠ±ã€‚Q-learning ä½¿ç”¨ Q å€¼å‡½æ•°
Q(s,a) æ¥è¡¨ç¤ºåœ¨çŠ¶æ€
s ä¸‹æ‰§è¡ŒåŠ¨ä½œ
a èƒ½è·å¾—çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ã€‚
#### 1.2.1 Q-learning
Q-learning æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå±äºå€¼å‡½æ•°æ–¹æ³•ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç§ç­–ç•¥ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨æ¯ä¸€ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©çš„åŠ¨ä½œèƒ½å¤Ÿæœ€å¤§åŒ–å…¶é¢„æœŸçš„ç´¯ç§¯å¥–åŠ±ã€‚

Q-learning é€šè¿‡å­¦ä¹ ä¸€ä¸ª Q å‡½æ•°ï¼ˆQ-table æˆ– Q-value functionï¼‰ï¼Œè¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªçŠ¶æ€å’Œä¸€ä¸ªåŠ¨ä½œä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªå€¼ï¼ˆQ å€¼ï¼‰ï¼Œè¡¨ç¤ºåœ¨è¿™ä¸ªçŠ¶æ€ä¸‹æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œèƒ½å¤Ÿè·å¾—çš„é¢„æœŸå¥–åŠ±ã€‚

Q-learning çš„æ›´æ–°å…¬å¼ä¸ºï¼š
<p align="center">
  <img src="Q-learning.png">
</p>
å…¶ä¸­ï¼š

* s æ˜¯å½“å‰çŠ¶æ€
* a æ˜¯å½“å‰åŠ¨ä½œ
* r æ˜¯æ‰§è¡ŒåŠ¨ä½œ a åå¾—åˆ°çš„å¥–åŠ±
* s' æ˜¯æ‰§è¡ŒåŠ¨ä½œ a åçš„ä¸‹ä¸€çŠ¶æ€
* Î± æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ›´æ–°é€Ÿåº¦
* Î³ æ˜¯æŠ˜æ‰£å› å­ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„é‡è¦æ€§

ä½†æ˜¯åœ¨æ™®é€šçš„Q-learningä¸­ï¼Œå½“çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£ä¸”ç»´æ•°ä¸é«˜æ—¶å¯ä½¿ç”¨Q-Tableå‚¨å­˜æ¯ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹çš„Qå€¼ï¼Œè€Œå½“çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ˜¯é«˜ç»´è¿ç»­æ—¶ï¼Œä½¿ç”¨Q-Tableä¸åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€å¤ªå¤§ååˆ†å›°éš¾ã€‚
å› æ­¤ï¼ŒDQNåœ¨Q-learningçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚
#### 1.2.2 æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰
DQN ä½¿ç”¨ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆQ ç½‘ç»œï¼‰æ¥é€¼è¿‘ Q å€¼å‡½æ•°ã€‚ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯çŠ¶æ€ ğ‘  ï¼Œè¾“å‡ºæ˜¯æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„ Q å€¼ 
ğ‘„(ğ‘ ,ğ‘;ğœƒ)ï¼Œå…¶ä¸­ ğœƒ æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼ŒDQN å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š

* ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰ï¼šæ™ºèƒ½ä½“åœ¨ä¸ç¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ä¸­ï¼Œä¼šç”Ÿæˆå¤§é‡çš„ç»éªŒæ•°æ®ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼‰ã€‚è¿™äº›æ•°æ®è¢«å­˜å‚¨åœ¨ä¸€ä¸ªå›ºå®šå¤§å°çš„å›æ”¾ç¼“å†²åŒºä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDQN ä¼šéšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„ç»éªŒè¿›è¡Œè®­ç»ƒã€‚è¿™ç§åšæ³•æ‰“ç ´äº†ç»éªŒä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

* ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰ï¼šDQN ç»´æŠ¤ä¸¤ä¸ªç¥ç»ç½‘ç»œï¼šå½“å‰ Q ç½‘ç»œï¼ˆç”¨äºé€‰æ‹©åŠ¨ä½œå’Œæ›´æ–° Q å€¼ï¼‰å’Œç›®æ ‡ Q ç½‘ç»œï¼ˆç”¨äºè®¡ç®—ç›®æ ‡ Q å€¼ï¼‰ã€‚ç›®æ ‡ Q ç½‘ç»œçš„å‚æ•°æ¯éš”ä¸€å®šæ­¥æ•°ä»å½“å‰ Q ç½‘ç»œå¤åˆ¶è€Œæ¥ï¼Œé¿å…äº†ç½‘ç»œå‚æ•°é¢‘ç¹æ›´æ–°å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚

### 1.3 å·¥ä½œæµç¨‹
1. **åˆå§‹åŒ–ï¼š**
   * åˆå§‹åŒ–å½“å‰ Q ç½‘ç»œå’Œç›®æ ‡ Q ç½‘ç»œï¼Œå¹¶éšæœºåˆå§‹åŒ–å®ƒä»¬çš„å‚æ•°ã€‚
   * åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒºä¸ºç©ºã€‚
2. **ä¸ç¯å¢ƒäº¤äº’ï¼š**
   * åœ¨æ¯ä¸ªæ—¶é—´æ­¥ ğ‘¡ ï¼Œæ™ºèƒ½ä½“æ ¹æ®å½“å‰ Q ç½‘ç»œçš„è¾“å‡ºå’Œ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œã€‚å³ï¼Œä»¥ Îµ çš„æ¦‚ç‡é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ï¼Œä»¥ 
   1 âˆ’ ğœ– çš„æ¦‚ç‡é€‰æ‹©å½“å‰ Q ç½‘ç»œè¾“å‡ºçš„æœ€å¤§ Q å€¼å¯¹åº”çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚
   * æ‰§è¡ŒåŠ¨ä½œ ğ‘ ï¼Œè·å¾—å¥–åŠ± 
   ğ‘Ÿ å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ ğ‘ â€²ã€‚
   * å°†ç»éªŒ(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒºã€‚
3. **ç»éªŒå›æ”¾ï¼š**
   * ä»å›æ”¾ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„ç»éªŒ(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)
   è¿›è¡Œè®­ç»ƒã€‚
   * å¯¹äºæ¯ä¸ªç»éªŒï¼Œè®¡ç®—ç›®æ ‡ Q å€¼ ğ‘¦ ï¼š
<p align="center">
  <img src="DQN.png">
</p>
    å…¶ä¸­ï¼Œ
ğ‘„(ğ‘ â€²,ğ‘â€²;ğœƒâˆ’)æ˜¯ç›®æ ‡ç½‘ç»œç»™å‡ºçš„ Q å€¼ï¼Œğœƒâˆ’è¡¨ç¤ºç›®æ ‡ç½‘ç»œçš„å‚æ•°

4. **æ›´æ–° Q ç½‘ç»œï¼š**
   * ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ–å½“å‰ Q ç½‘ç»œè¾“å‡ºçš„ Q å€¼ä¸ç›®æ ‡ Q å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š
<p align="center">
  <img src="MSE.png">
</p>
    * è®¡ç®—æŸå¤±å¹¶æ›´æ–°å½“å‰ Q ç½‘ç»œçš„å‚æ•°ğœƒã€‚

5. **æ›´æ–°ç›®æ ‡ç½‘ç»œï¼š**
   * æ¯éš”ä¸€å®šæ­¥æ•°ï¼Œå°†å½“å‰ Q ç½‘ç»œçš„å‚æ•° ğœƒ å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œçš„å‚æ•° ğœƒâˆ’ ã€‚

6. **é‡å¤ï¼š**
   * é‡å¤æ­¥éª¤ 2 åˆ°æ­¥éª¤ 5ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°æˆ–å…¶ä»–ç»ˆæ­¢æ¡ä»¶ã€‚

## 2 æœ¬åœ°éƒ¨ç½²

**æ–‡æ¡£è¯´æ˜ï¼š**
* main.py #ä¸»æ–‡ä»¶
* train.py #æ¨¡å‹è®­ç»ƒ
* evaluate.py #æ¨¡å‹è¯„ä¼°
* visualize.py #å¯è§†åŒ–
* ReadMe.md #æ–‡æ¡£

### 2.1 è™šæ‹Ÿç¯å¢ƒé…ç½®
```
conda create -n env_nameï¼ˆç¯å¢ƒåç§°å¯è‡ªè¡Œä¿®æ”¹ï¼‰ python=3.8 # å®‰è£…è™šæ‹Ÿç¯å¢ƒ
activate env_name
pip install highway-env==1.8.2
pip install gymnasium==0.29.1
```
### 2.2 å…·ä½“æ­¥éª¤
#### 2.2.1 æ¨¡å‹è®­ç»ƒ
```
def train_model():
    # DQN ç½‘ç»œæ¨¡å‹å®šä¹‰
    class DQN(nn.Module):
        def __init__(self, input_size, output_size):
            super(DQN, self).__init__()
            self.fc1 = nn.Linear(input_size, 64)  # ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚
            self.fc2 = nn.Linear(64, 64)          # ç¬¬äºŒå±‚å…¨è¿æ¥å±‚
            self.fc3 = nn.Linear(64, output_size) # è¾“å‡ºå±‚

        def forward(self, x):
            x = torch.relu(self.fc1(x))  # æ¿€æ´»å‡½æ•° ReLU
            x = torch.relu(self.fc2(x))  # æ¿€æ´»å‡½æ•° ReLU
            x = self.fc3(x)              # è¾“å‡ºå±‚
            return x

    # è®¾ç½®ç¯å¢ƒå’Œè¶…å‚æ•°
    env = gym.make("intersection-v0", render_mode='human')  # åˆ›å»ºç¯å¢ƒ
    state_size = env.observation_space.shape[0] * env.observation_space.shape[1]  # è¾“å…¥ç‰¹å¾å¤§å°
    action_size = env.action_space.n  # åŠ¨ä½œç©ºé—´çš„å¤§å°
    batch_size = 32                   # æ‰¹æ¬¡å¤§å°
    gamma = 0.99                      # æŠ˜æ‰£å› å­
    epsilon = 1.0                     # è´ªå©ªç­–ç•¥çš„åˆå§‹å€¼
    epsilon_min = 0.01                # è´ªå©ªç­–ç•¥çš„æœ€å°å€¼
    epsilon_decay = 0.995             # è´ªå©ªç­–ç•¥çš„è¡°å‡å› å­
    learning_rate = 0.001             # å­¦ä¹ ç‡
    target_model_update_freq = 10     # ç›®æ ‡æ¨¡å‹æ›´æ–°é¢‘ç‡
    memory = deque(maxlen=2000)       # ç»éªŒå›æ”¾é˜Ÿåˆ—

    model = DQN(state_size, action_size)  # åˆ›å»ºæ¨¡å‹
    target_model = DQN(state_size, action_size)  # åˆ›å»ºç›®æ ‡æ¨¡å‹
    target_model.load_state_dict(model.state_dict())  # åˆå§‹åŒ–ç›®æ ‡æ¨¡å‹
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # ä¼˜åŒ–å™¨

    # é€‰æ‹©åŠ¨ä½œçš„ç­–ç•¥
    def choose_action(state):
        if np.random.rand() <= epsilon:  # è´ªå©ªç­–ç•¥
            return random.randrange(action_size)  # éšæœºé€‰æ‹©
        state = torch.FloatTensor(state).unsqueeze(0)  # è½¬æ¢ä¸º tensor
        state = state.view(state.size(0), -1)  # è°ƒæ•´ç»´åº¦
        q_values = model(state)  # è·å– Q å€¼
        return torch.argmax(q_values).item()  # è¿”å› Q å€¼æœ€å¤§å¯¹åº”çš„åŠ¨ä½œ

    # ç»éªŒå›æ”¾å’Œè®­ç»ƒè¿‡ç¨‹
    def experience_replay():
        if len(memory) < batch_size:  # å¦‚æœè®°å¿†ä¸è¶³ä»¥è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼Œåˆ™è·³è¿‡
            return
        batch = random.sample(memory, batch_size)  # ä»è®°å¿†ä¸­éšæœºæŠ½å–ä¸€æ‰¹
        states, actions, rewards, next_states, dones = zip(*batch)
        states = torch.FloatTensor(states).view(batch_size, -1)  # çŠ¶æ€
        actions = torch.LongTensor(actions).unsqueeze(1)  # åŠ¨ä½œ
        rewards = torch.FloatTensor(rewards)  # å¥–åŠ±
        next_states = torch.FloatTensor(next_states).view(batch_size, -1)  # ä¸‹ä¸€ä¸ªçŠ¶æ€
        dones = torch.BoolTensor(dones)  # ç»“æŸæ ‡å¿—

        current_q_values = model(states).gather(1, actions).squeeze(1)  # å½“å‰ Q å€¼
        with torch.no_grad():
            next_q_values = target_model(next_states).max(1)[0]  # ä¸‹ä¸€çŠ¶æ€çš„ Q å€¼
        target_q_values = rewards + (gamma * next_q_values * ~dones)  # ç›®æ ‡ Q å€¼

        loss = nn.MSELoss()(current_q_values, target_q_values)  # è®¡ç®—æŸå¤±
        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°

    # è®­ç»ƒè¿‡ç¨‹
    episode_rewards = []  # è®°å½•æ¯ä¸ªå›åˆçš„å¥–åŠ±
    episode_epsilon = []  # è®°å½•æ¯ä¸ªå›åˆçš„ epsilon å€¼
    model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model")
    os.makedirs(model_dir, exist_ok=True)  # åˆ›å»ºæ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹

    episodes = 500  # è®­ç»ƒå›åˆæ•°
    for episode in range(episodes):
        state = env.reset()  # åˆå§‹åŒ–ç¯å¢ƒ
        total_reward = 0
        done = False
        while not done:
            action = choose_action(state)  # é€‰æ‹©åŠ¨ä½œ
            next_state, reward, done, _, info = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œ

            memory.append((state, action, reward, next_state, done))  # ä¿å­˜ç»éªŒ
            experience_replay()  # è¿›è¡Œç»éªŒå›æ”¾å’Œè®­ç»ƒ

            state = next_state  # æ›´æ–°çŠ¶æ€
            total_reward += reward  # ç´¯åŠ å¥–åŠ±

        if epsilon > epsilon_min:  # æ›´æ–° epsilon å€¼
            epsilon *= epsilon_decay

        if episode % target_model_update_freq == 0:  # æ›´æ–°ç›®æ ‡æ¨¡å‹
            target_model.load_state_dict(model.state_dict())

        episode_rewards.append(total_reward)
        episode_epsilon.append(epsilon)

        print(f"Episode {episode}/{episodes} - Total reward: {total_reward}, Epsilon: {epsilon}")

    # ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—
    torch.save(model.state_dict(), os.path.join(model_dir, 'dqn_model.pth'))
    with open(os.path.join(model_dir, 'training_log.txt'), 'w') as log_file:
        for reward, eps in zip(episode_rewards, episode_epsilon):
            log_file.write(f"Reward: {reward}, Epsilon: {eps}\n")

    print("Model and logs saved successfully!")
```
é€šè¿‡æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„äº¤é€šç¯å¢ƒä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒæ¨¡å‹æ¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œã€‚ã€‚
* å®šä¹‰æ¨¡å‹ï¼ˆDQNï¼‰ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ã€‚æ¨¡å‹æœ‰ä¸‰å±‚å…¨è¿æ¥å±‚ï¼ˆfc1, fc2, fc3ï¼‰ï¼Œä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ã€‚
* è®¾ç½®ç¯å¢ƒå’Œè¶…å‚æ•°ï¼šåˆ›å»ºä¸€ä¸ª intersection-v0 ç¯å¢ƒã€‚è¶…å‚æ•°å¦‚å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€epsilonï¼ˆç”¨äºÎµ-è´ªå©ªç­–ç•¥ï¼‰ç­‰å·²å®šä¹‰ã€‚
* é€‰æ‹©åŠ¨ä½œï¼ˆchoose_actionï¼‰ï¼šå¦‚æœ epsilon å€¼å¤§äºéšæœºå€¼ï¼Œåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ï¼›å¦åˆ™ï¼Œé€‰æ‹©æœ€å¤§ Q å€¼çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚
* ç»éªŒå›æ”¾ï¼ˆexperience_replayï¼‰ï¼šä»ç»éªŒå›æ”¾é˜Ÿåˆ—ä¸­éšæœºæŠ½å–ä¸€æ‰¹æ•°æ®ï¼Œå¹¶è¿›è¡Œ Q å€¼çš„æ›´æ–°ã€‚é€šè¿‡ MSE æŸå¤±è®¡ç®—é¢„æµ‹å€¼ä¸ç›®æ ‡ Q å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œå¹¶åå‘ä¼ æ’­æ›´æ–°ç½‘ç»œã€‚
* è®­ç»ƒå¾ªç¯ï¼šæ¯ä¸ªå›åˆä»ç¯å¢ƒé‡ç½®å¼€å§‹ï¼Œé€‰æ‹©åŠ¨ä½œå¹¶æ›´æ–°çŠ¶æ€ã€å¥–åŠ±ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¨¡å‹ä¼šæ›´æ–°ç›®æ ‡æ¨¡å‹çš„å‚æ•°ã€‚epsilon å€¼ä¼šé€æ­¥è¡°å‡ï¼Œå‡å°‘æ¢ç´¢çš„é¢‘ç‡ï¼Œå¢åŠ æ¨¡å‹çš„åˆ©ç”¨ã€‚
* ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—ï¼šè®­ç»ƒç»“æŸåï¼Œä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè®­ç»ƒæ—¥å¿—è‡³modelæ–‡ä»¶å¤¹ã€‚


#### 2.2.2 æ¨¡å‹è¯„ä¼°
```
def evaluate_model():
    class DQN(nn.Module):
        def __init__(self, input_size, output_size):
            super(DQN, self).__init__()
            self.fc1 = nn.Linear(input_size, 64)
            self.fc2 = nn.Linear(64, 64)
            self.fc3 = nn.Linear(64, output_size)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = self.fc3(x)
            return x

    env = gym.make("intersection-v0", render_mode='human')
    state_size = env.observation_space.shape[0] * env.observation_space.shape[1]
    action_size = env.action_space.n

    model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model")
    model = DQN(state_size, action_size)
    model.load_state_dict(torch.load(os.path.join(model_dir, 'dqn_model.pth')))
    model.eval()

    def choose_action(state):
        if isinstance(state, tuple):
            state = state[0]
        state = torch.FloatTensor(state).unsqueeze(0)
        state = state.view(state.size(0), -1)
        q_values = model(state)
        return torch.argmax(q_values).item()

    def evaluate():
        total_reward = 0
        state = env.reset()
        done = False
        while not done:
            action = choose_action(state)
            next_state, reward, done, _, info = env.step(action)
            state = next_state
            total_reward += reward

        print(f"Total reward in evaluation: {total_reward}")

    evaluate()
```
evaluate.py çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¹¶åœ¨ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚ï¼ˆæ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€ä¼˜çš„åŠ¨ä½œé€‰æ‹©ç­–ç•¥
ï¼ˆå³ Q å€¼æœ€å¤§åŒ–ï¼‰æ¥è¿›è¡Œè¯„ä¼°ï¼Œä¸æ¶‰åŠ epsilon-è´ªå©ªç­–ç•¥çš„éšæœºæ€§ã€‚ï¼‰
* å®šä¹‰æ¨¡å‹ï¼ˆDQNï¼‰ï¼šDQN ç±»å’Œ train.py ä¸­å®šä¹‰çš„æ¨¡å‹ç›¸åŒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰å±‚å…¨è¿æ¥å±‚ï¼ˆfc1, fc2, fc3ï¼‰æ¥æ„å»ºç¥ç»ç½‘ç»œï¼Œæœ€ç»ˆè¾“å‡º Q å€¼ã€‚
è¿™ä¸ªæ¨¡å‹ç»“æ„ä¸è®­ç»ƒé˜¶æ®µç›¸åŒï¼Œç¡®ä¿è¯„ä¼°æ—¶ä½¿ç”¨çš„æ˜¯ä¸è®­ç»ƒé˜¶æ®µç›¸åŒçš„æ¶æ„ã€‚
* åˆ›å»ºç¯å¢ƒï¼šä½¿ç”¨ gym.make() åˆ›å»ºä¸€ä¸ª intersection-v0 ç¯å¢ƒï¼Œstate_size å’Œ action_size ç”¨äºç¡®å®šæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼Œåˆ†åˆ«è¡¨ç¤ºçŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„å¤§å°ã€‚
* åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼šé€šè¿‡ torch.load() åŠ è½½è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æ¨¡å‹æƒé‡ï¼Œå¹¶å°†å…¶åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚
* é€‰æ‹©åŠ¨ä½œï¼šä¸è®­ç»ƒæ—¶ç›¸åŒï¼Œåœ¨è¯„ä¼°é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ epsilon-è´ªå©ªç­–ç•¥æ¥é€‰æ‹©åŠ¨ä½œã€‚è¿™é‡Œæˆ‘ä»¬åªæ‰§è¡Œè´ªå©ªç­–ç•¥ï¼Œå³é€‰æ‹©å½“å‰ Q å€¼æœ€å¤§çš„åŠ¨ä½œã€‚
* è¯„ä¼°è¿‡ç¨‹ï¼ševaluate() å‡½æ•°ä¸­ï¼Œåˆå§‹åŒ–ç¯å¢ƒå¹¶è®©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œã€‚æ¯ä¸€æ­¥ä¸­ï¼Œæ™ºèƒ½ä½“é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œå¹¶æ›´æ–°çŠ¶æ€ï¼Œç´¯ç§¯å¥–åŠ±ç›´åˆ°å›åˆç»“æŸã€‚æœ€ç»ˆè¾“å‡ºè¯„ä¼°è¿‡ç¨‹ä¸­çš„æ€»å¥–åŠ±ã€‚

#### 2.2.3 æ¨¡å‹é¢„æµ‹ä¸å¯è§†åŒ–
```
# ç®€å•çš„ç§»åŠ¨å¹³å‡å‡½æ•°
def moving_average(data, window_size):
    return [sum(data[i:i+window_size]) / window_size for i in range(len(data) - window_size + 1)]

def visualize_results():
    model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "model")

    episode_rewards = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªå›åˆçš„æ€»å¥–åŠ±
    episode_epsilon = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªå›åˆçš„epsilonå€¼

    # ä»è®­ç»ƒæ—¥å¿—æ–‡ä»¶ä¸­è¯»å–å¥–åŠ±å’Œepsilonå€¼
    with open(os.path.join(model_dir, 'training_log.txt'), 'r') as log_file:
        for line in log_file:
            reward, epsilon = line.strip().split(", ")  # åˆ†å‰²æ¯ä¸€è¡Œçš„å¥–åŠ±å’Œepsilon
            episode_rewards.append(float(reward.split(": ")[1]))  # å­˜å‚¨å¥–åŠ±å€¼
            episode_epsilon.append(float(epsilon.split(": ")[1]))  # å­˜å‚¨epsilonå€¼

    # å¯¹å¥–åŠ±è¿›è¡Œå¹³æ»‘å¤„ç†
    smoothed_rewards = moving_average(episode_rewards, window_size=10)  # ä½¿ç”¨çª—å£å¤§å°ä¸º10çš„ç§»åŠ¨å¹³å‡

    # åˆ›å»ºä¸€ä¸ªå›¾å½¢çª—å£
    plt.figure(figsize=(12, 6))

    # ç»˜åˆ¶ç¬¬ä¸€ä¸ªå­å›¾ï¼šæ¯ä¸ªå›åˆçš„å¥–åŠ±ï¼ˆå¹³æ»‘åçš„ï¼‰
    plt.subplot(1, 2, 1)
    plt.plot(smoothed_rewards, label='Smoothed Reward')
    plt.title('Smoothed Total Reward per Episode')
    plt.xlabel('Episode')
    plt.ylabel('Smoothed Total Reward')

    # ç»˜åˆ¶ç¬¬äºŒä¸ªå­å›¾ï¼šepsilonçš„è¡°å‡
    plt.subplot(1, 2, 2)
    plt.plot(episode_epsilon, label='Epsilon', color='r')
    plt.title('Epsilon Decay per Episode')
    plt.xlabel('Episode')
    plt.ylabel('Epsilon')

    # è°ƒæ•´å›¾åƒçš„å¸ƒå±€ï¼Œä½¿å¾—å›¾å½¢ä¸ä¼šé‡å 
    plt.tight_layout()

    # æ˜¾ç¤ºå›¾å½¢
    plt.show()
```
visualize.py æ–‡ä»¶çš„ä½œç”¨æ˜¯ç›´è§‚åœ°çœ‹åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„å­¦ä¹ è¿›å±•ï¼ŒåŒ…æ‹¬å¥–åŠ±çš„å˜åŒ–å’Œ epsilon çš„è¡°å‡ã€‚
* å®šä¹‰ moving_average(data, window_size) å‡½æ•°ï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡å¯¹å¥–åŠ±è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´æ¸…æ™°åœ°çœ‹åˆ°å¥–åŠ±éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ï¼Œå»é™¤å•ä¸ªå›åˆçš„éšæœºæ³¢åŠ¨ã€‚
window_size æ˜¯æ§åˆ¶å¹³æ»‘ç¨‹åº¦çš„å‚æ•°ï¼Œçª—å£å¤§å°è¶Šå¤§ï¼Œå¹³æ»‘æ•ˆæœè¶Šæ˜æ˜¾ï¼Œä½†å¯èƒ½ä¼šä½¿æ›²çº¿å¤±å»ä¸€äº›ç»†èŠ‚ã€‚å¦‚æœæƒ³è¦æ›´ç»†è…»çš„å¥–åŠ±æ³¢åŠ¨ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨è¾ƒå°çš„çª—å£å¤§å°ï¼Œä¾‹å¦‚ 5 æˆ– 3ã€‚
* å®šä¹‰ visualize_results å‡½æ•°ï¼šè¯¥å‡½æ•°çš„ä¸»è¦ä»»åŠ¡æ˜¯ä»ä¿å­˜çš„æ—¥å¿—æ–‡ä»¶ä¸­è¯»å–è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªå›åˆçš„å¥–åŠ±å€¼å’Œ epsilon å€¼ï¼Œå¹¶ç»˜åˆ¶å®ƒä»¬çš„å›¾è¡¨ã€‚ 
* å¥–åŠ±ï¼ˆRewardï¼‰ï¼šæ¯ä¸ªå›åˆä¸­æ™ºèƒ½ä½“è·å¾—çš„æ€»å¥–åŠ±ã€‚ 
* Epsilonï¼šåœ¨ epsilon-è´ªå©ªç­–ç•¥ä¸­ï¼Œepsilon æ§åˆ¶éšæœºé€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œepsilon é€šå¸¸ä¼šé€æ¸è¡°å‡ã€‚
* è¯»å–è®­ç»ƒæ—¥å¿—æ–‡ä»¶ï¼šmodel_dirä¸ºè·å–ä¿å­˜è®­ç»ƒæ¨¡å‹å’Œæ—¥å¿—æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
ä½¿ç”¨ open() å‡½æ•°æ‰“å¼€ training_log.txt æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶ä¸­è®°å½•äº†æ¯ä¸ªå›åˆçš„å¥–åŠ±å’Œ epsilon å€¼ã€‚
* ç»˜åˆ¶å›¾è¡¨ï¼šä½¿ç”¨ plt.figure(figsize=(12, 6)) åˆ›å»ºä¸€ä¸ªå›¾å½¢çª—å£ï¼Œå¹¶è®¾ç½®çª—å£çš„å¤§å°ã€‚
ä½¿ç”¨ plt.plot() ç»˜åˆ¶æ¯ä¸ªå›åˆçš„å¥–åŠ±æ›²çº¿å’Œ epsilon çš„è¡°å‡æ›²çº¿ï¼Œå¹¶æ·»åŠ ç›¸åº”çš„æ ‡é¢˜ã€æ ‡ç­¾å’Œé¢œè‰²ã€‚
ä½¿ç”¨ plt.show() æ˜¾ç¤ºæœ€ç»ˆç»˜åˆ¶çš„å›¾å½¢ã€‚


## 3 å®Œæ•´Demo
```
import sys
from utils.train import train_model
from utils.evaluate import evaluate_model
from utils.visualize import visualize_results

def main():
    print("Select an option:")
    print("1. Train the model")
    print("2. Evaluate the model")
    print("3. Visualize the results")

    choice = input("Enter the number of your choice: ")

    if choice == '1':
        print("Training the model...")
        train_model()
    elif choice == '2':
        print("Evaluating the model...")
        evaluate_model()
    elif choice == '3':
        print("Visualizing the results...")
        visualize_results()
    else:
        print("Invalid choice. Exiting.")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

ä»¿çœŸç¯å¢ƒå¦‚ä¸‹å›¾ï¼š
<p align="center">
  <img src="intersection.png">
</p>

æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
<p align="center">
  <img src="img.png">
</p>

åœ¨intersection-v0ç¯å¢ƒä¸‹è¿›è¡Œ2000æ¬¡è®­ç»ƒçš„å¥–åŠ±æ›²çº¿å’Œ epsilon çš„è¡°å‡æ›²çº¿å¦‚ä¸‹æ‰€ç¤ºï¼š
<p align="center">
  <img src="demo.png">
</p>
